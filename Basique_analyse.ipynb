{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"hn2018_2019.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dater(datetim,integer):\n",
    "    date,time = datetim.split(\" \")\n",
    "    year,month,day = date.split(\"-\")\n",
    "    hour,minute,second = time.split(\":\")\n",
    "    second = int(second)+ int(minute)*60 + int(hour)*60*60\n",
    "    if integer==1:\n",
    "        return year\n",
    "    elif integer==2:\n",
    "        return month\n",
    "    elif integer==3:\n",
    "        return day\n",
    "    else:\n",
    "        return second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"year\"] = data['Created At'].map(lambda x: dater(x,1))\n",
    "data[\"month\"] = data['Created At'].map(lambda x: dater(x,2))\n",
    "data[\"day\"] = data['Created At'].map(lambda x: dater(x,3))\n",
    "data[\"second\"] = data['Created At'].map(lambda x: dater(x,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018 = data[data[\"year\"]=='2018'][['Title','Post Type']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018['Title'] = df_2018['Title'].map(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a column of tokenized words in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018['tokenized_title'] = df_2018['Title'].map(lambda x:re.split('\\[\\^a-zA-Z\\]',x)[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating frequency of each word and given their conditional post type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_phrase(phrase,typ,dict_words):\n",
    "    for i in phrase:\n",
    "        if i not in dict_words:\n",
    "            dict_words[i] = {'story':0,'ask_hn':0,'show_hn':0,'poll':0}\n",
    "        dict_words[i][typ] += 1\n",
    "    return dict_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "num = 0\n",
    "for i in range(len(df_2018['tokenized_title'])):\n",
    "    d = input_phrase(df_2018['tokenized_title'][i],df_2018['Post Type'][i],d)\n",
    "    num += len(df_2018['tokenized_title'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_2018 = pd.DataFrame(d)\n",
    "words_2018 = words_2018.transpose()\n",
    "words_2018.index.name = 'TokenName'\n",
    "words_2018 = words_2018.reset_index()\n",
    "words_2018 = words_2018.sort_values(by ='TokenName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_2018['prob_ask_hn'] = words_2018['ask_hn'].map(lambda x: (int(x)+delta)/(num+delta*len(words_2018)))\n",
    "words_2018['prob_poll'] = words_2018['poll'].map(lambda x: (int(x)+delta)/(num+delta*len(words_2018)))\n",
    "words_2018['prob_show_hn'] = words_2018['show_hn'].map(lambda x: (int(x)+delta)/(num+delta*len(words_2018)))\n",
    "words_2018['prob_story'] = words_2018['story'].map(lambda x: (int(x)+delta)/(num+delta*len(words_2018)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"model-2018.txt\",\"w+\")\n",
    "count = 0\n",
    "for index in words_2018.index:\n",
    "    line = str(count + 1) + '  '\n",
    "    line += str(words_2018['TokenName'][index]) + '  '\n",
    "    line += str(words_2018['story'][index]) + '  '\n",
    "    line += str(words_2018['prob_story'][index]) + '  '\n",
    "    line += str(words_2018['ask_hn'][index]) + '  '\n",
    "    line += str(words_2018['prob_ask_hn'][index]) + '  '\n",
    "    line += str(words_2018['show_hn'][index]) + '  '\n",
    "    line += str(words_2018['prob_show_hn'][index]) + '  '\n",
    "    line += str(words_2018['poll'][index]) + '  '\n",
    "    line += str(words_2018['prob_poll'][index]) + '\\n'\n",
    "    f.write(line)\n",
    "    count += 1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priorProbabilities = {'story':0,'ask_hn':0,'show_hn':0,'poll':0}\n",
    "unique, counts = np.unique(df_2018['Post Type'], return_counts=True)\n",
    "for index in range(len(unique)):\n",
    "    priorProbabilities[unique[index]] = counts[index]/np.sum(counts)\n",
    "print(priorProbabilities)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "conditionalProbabilities = {}\n",
    "for index in words_2018.index:\n",
    "    TokenName = words_2018['TokenName'][index]\n",
    "    TokenConditionalProbability = {'story':words_2018['prob_story'][index],\n",
    "                                   'ask_hn':words_2018['prob_ask_hn'][index],\n",
    "                                   'show_hn':words_2018['prob_show_hn'][index],\n",
    "                                   'poll':words_2018['prob_poll'][index]}\n",
    "    conditionalProbabilities[TokenName] = TokenConditionalProbability\n",
    "    break\n",
    "print(conditionalProbabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(words_2018))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = data[data[\"year\"]=='2019'][['Title','Post Type']].copy()\n",
    "testData['Title'] = testData['Title'].map(lambda x:x.lower())\n",
    "testData['tokenized_title'] = testData['Title'].map(lambda x:re.split('\\[\\^a-zA-Z\\]',x)[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
