{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import nltk\n",
    "\n",
    "from generateWordFrequency import *\n",
    "from naiveBayes import *\n",
    "from fileWriteFunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penn_to_wn(tag):\n",
    "    nltk_wn_pos = {'J':wn.ADJ,'V':wn.VERB,'N':wn.NOUN,'R':wn.ADV}\n",
    "    try:\n",
    "        return nltk_wn_pos[tag[0]]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018 = getDataframe(2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. lowercase\n",
    "2. tokenize with pos\n",
    "3. remove punctuations\n",
    "4. combine NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penn_to_wn(tag):\n",
    "    nltk_wn_pos = {'J':wn.ADJ,'V':wn.VERB,'N':wn.NOUN,'R':wn.ADV}\n",
    "    try:\n",
    "        return nltk_wn_pos[tag[0]]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def checkPunctuation(word):\n",
    "    punctuations = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/','”','“','–',\"'s\",\n",
    "                ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
    "    if word in punctuations:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def tokenizeSentence(sentence):\n",
    "    tokenized = []\n",
    "    text = nltk.word_tokenize(sentence)\n",
    "    grab = False\n",
    "    print(nltk.pos_tag(text))\n",
    "    for word,pos in nltk.pos_tag(text):\n",
    "        if checkPunctuation(word):\n",
    "            continue\n",
    "        tag = penn_to_wn(pos)\n",
    "        if tag is None:\n",
    "            lemmatizedWord = word\n",
    "        else:\n",
    "            lemmatizedWord = wordnet_lemmatizer.lemmatize(word,tag)\n",
    "        lemmatizedWord = lemmatizedWord.lower()\n",
    "        if pos=='NNP':\n",
    "            if grab:\n",
    "                tokenized[-1] += \" \" + lemmatizedWord\n",
    "                grab = False\n",
    "                continue\n",
    "            else:\n",
    "                grab = True\n",
    "        else:\n",
    "            if grab:\n",
    "                grab = False\n",
    "        tokenized.append(lemmatizedWord)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data', 'NNS'), ('science', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('subject', 'NN'), ('i', 'NN'), ('chose', 'VBP'), ('computer', 'NN'), ('science', 'NN'), ('as', 'IN'), ('my', 'PRP$'), ('major', 'JJ'), ('ask_hn', 'NN')]\n",
      "['data', 'science', 'be', 'a', 'subject', 'i', 'choose', 'computer', 'science', 'as', 'my', 'major', 'ask_hn']\n",
      "[('Data', 'NNP'), ('Science', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('subject', 'NN'), ('I', 'PRP'), ('chose', 'VBP'), ('Computer', 'NNP'), ('Science', 'NNP'), ('as', 'IN'), ('my', 'PRP$'), ('major', 'JJ'), ('Ask_HN', 'NNP')]\n",
      "['data science', 'be', 'a', 'subject', 'i', 'choose', 'computer science', 'as', 'my', 'major', 'ask_hn']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizeSentence('Data Science is a subject I chose Computer Science as my major Ask_HN'.lower()))\n",
    "print(tokenizeSentence('Data Science is a subject I chose Computer Science as my major Ask_HN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The Three-Body Problem' could be Amazon's next global blockbuster\n",
      "[(\"'The\", 'POS'), ('Three-Body', 'NN'), ('Problem', 'NNP'), (\"'\", 'POS'), ('could', 'MD'), ('be', 'VB'), ('Amazon', 'NNP'), (\"'s\", 'POS'), ('next', 'JJ'), ('global', 'JJ'), ('blockbuster', 'NN')]\n",
      "[\"'the\", 'three-body', 'problem', 'could', 'be', 'amazon', 'next', 'global', 'blockbuster']\n",
      "Show HN: Riter – Agile-oriented project management software\n",
      "[('Show', 'NNP'), ('HN', 'NNP'), (':', ':'), ('Riter', 'NNP'), ('–', 'NNP'), ('Agile-oriented', 'NNP'), ('project', 'NN'), ('management', 'NN'), ('software', 'NN')]\n",
      "['show hn', 'riter agile-oriented', 'project', 'management', 'software']\n",
      "The Flatness of U.S. States [pdf]\n",
      "[('The', 'DT'), ('Flatness', 'NNP'), ('of', 'IN'), ('U.S.', 'NNP'), ('States', 'NNPS'), ('[', 'NNP'), ('pdf', 'NN'), (']', 'NN')]\n",
      "['the', 'flatness', 'of', 'u.s.', 'states', 'pdf']\n",
      "A Tale of Two Asyncs: Open Source Language Design in Rust and Node.js\n",
      "[('A', 'DT'), ('Tale', 'NNP'), ('of', 'IN'), ('Two', 'CD'), ('Asyncs', 'NNS'), (':', ':'), ('Open', 'NNP'), ('Source', 'NNP'), ('Language', 'NNP'), ('Design', 'NNP'), ('in', 'IN'), ('Rust', 'NNP'), ('and', 'CC'), ('Node.js', 'NNP')]\n",
      "['a', 'tale', 'of', 'two', 'asyncs', 'open source', 'language design', 'in', 'rust', 'and', 'node.js']\n",
      "Claim of first Human babies' genome modified by CRISPR\n",
      "[('Claim', 'NN'), ('of', 'IN'), ('first', 'JJ'), ('Human', 'NNP'), ('babies', 'NNS'), (\"'\", 'POS'), ('genome', 'NN'), ('modified', 'VBN'), ('by', 'IN'), ('CRISPR', 'NNP')]\n",
      "['claim', 'of', 'first', 'human', 'baby', 'genome', 'modify', 'by', 'crispr']\n",
      "Is It Easier to Imagine the End of the World Than the End of the Internet?\n",
      "[('Is', 'VBZ'), ('It', 'PRP'), ('Easier', 'JJR'), ('to', 'TO'), ('Imagine', 'VB'), ('the', 'DT'), ('End', 'NN'), ('of', 'IN'), ('the', 'DT'), ('World', 'NNP'), ('Than', 'NNP'), ('the', 'DT'), ('End', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Internet', 'NNP'), ('?', '.')]\n",
      "['is', 'it', 'easier', 'to', 'imagine', 'the', 'end', 'of', 'the', 'world than', 'the', 'end', 'of', 'the', 'internet']\n",
      "Virtualized iOS builds at Shopify\n",
      "[('Virtualized', 'VBN'), ('iOS', 'JJ'), ('builds', 'NNS'), ('at', 'IN'), ('Shopify', 'NNP')]\n",
      "['virtualized', 'ios', 'build', 'at', 'shopify']\n",
      "Ask HN: What modern CSS framework do you use?\n",
      "[('Ask', 'NNP'), ('HN', 'NNP'), (':', ':'), ('What', 'WP'), ('modern', 'JJ'), ('CSS', 'NNP'), ('framework', 'NN'), ('do', 'VBP'), ('you', 'PRP'), ('use', 'VB'), ('?', '.')]\n",
      "['ask hn', 'what', 'modern', 'css', 'framework', 'do', 'you', 'use']\n",
      "Sleeping Rough in the Mattress Economy\n",
      "[('Sleeping', 'VBG'), ('Rough', 'NNP'), ('in', 'IN'), ('the', 'DT'), ('Mattress', 'NNP'), ('Economy', 'NNP')]\n",
      "['sleeping', 'rough', 'in', 'the', 'mattress economy']\n",
      "Bird sues Beverly Hills over its ban of electric scooters\n",
      "[('Bird', 'NNP'), ('sues', 'NNS'), ('Beverly', 'NNP'), ('Hills', 'NNPS'), ('over', 'IN'), ('its', 'PRP$'), ('ban', 'NN'), ('of', 'IN'), ('electric', 'JJ'), ('scooters', 'NNS')]\n",
      "['bird', 'sue', 'beverly', 'hills', 'over', 'its', 'ban', 'of', 'electric', 'scooter']\n"
     ]
    }
   ],
   "source": [
    "for sent in df_2018['Title'][np.random.randint(len(df_2018), size=10)]:\n",
    "    print(sent)\n",
    "    print(tokenizeSentence(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Data', 'NNP'), ('Science', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('subject', 'NN')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data science', 'be', 'a', 'subject']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizeSentence(\"Data Science is a subject\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Data', 'NNP'), ('Science', 'NNP'), ('IS', 'NNP'), ('A', 'NNP'), ('Subject', 'JJ')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data science', 'is a', 'subject']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizeSentence(\"Data Science IS A Subject\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data', 'NNS'), ('science', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('subject', 'NN')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data', 'science', 'be', 'a', 'subject']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizeSentence(\"Data Science is a subject\".lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Data', 'NNP'), ('Science', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('subject', 'NN'), ('I', 'PRP'), ('chose', 'VBP'), ('Computer', 'NNP'), ('Science', 'NNP'), ('as', 'IN'), ('my', 'PRP$'), ('major', 'JJ'), ('Ask_HN', 'NNP')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data science',\n",
       " 'be',\n",
       " 'a',\n",
       " 'subject',\n",
       " 'i',\n",
       " 'choose',\n",
       " 'computer science',\n",
       " 'as',\n",
       " 'my',\n",
       " 'major',\n",
       " 'ask_hn']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizeSentence('Data Science is a subject I chose Computer Science as my major Ask_HN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Data', 'NNP'), ('Science', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('subject', 'NN'), ('I', 'PRP'), ('Chose', 'NNP'), ('Computer', 'NNP'), ('Science', 'NNP'), ('AS', 'IN'), ('My', 'NNP'), ('Major', 'NNP'), ('Ask_HN', 'NNP')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data science',\n",
       " 'be',\n",
       " 'a',\n",
       " 'subject',\n",
       " 'i',\n",
       " 'chose computer',\n",
       " 'science',\n",
       " 'as',\n",
       " 'my major',\n",
       " 'ask_hn']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizeSentence('Data Science is a subject I Chose Computer Science AS My Major Ask_HN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
