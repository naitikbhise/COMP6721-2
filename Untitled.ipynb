{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/fracton/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "from nltk import DefaultTagger, UnigramTagger, BigramTagger\n",
    "train_sents = brown.tagged_sents()\n",
    "#unigram_tagger = nltk.UnigramTagger(train_sents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from generateWordFrequency import *\n",
    "from naiveBayes import *\n",
    "from fileWriteFunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_train = []\n",
    "for sent in train_sents:\n",
    "    edited_train.append([(word.lower(),tag) for (word,tag) in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = DefaultTagger('NN')\n",
    "t1 = UnigramTagger(train_sents, backoff = t0)\n",
    "t2 = BigramTagger(train_sents, backoff = t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#et0 = DefaultTagger('NNP')\n",
    "et1 = UnigramTagger(edited_train, backoff = None)\n",
    "et2 = BigramTagger(edited_train, backoff = et1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'NN'),\n",
       " ('like', 'CS'),\n",
       " ('data', 'NNS'),\n",
       " ('science', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('computer', 'NN'),\n",
       " ('science', 'NN'),\n",
       " ('as', 'CS'),\n",
       " ('my', 'PP$'),\n",
       " ('school', 'NN'),\n",
       " ('subjects', 'NNS')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.tag(nltk.word_tokenize(\"I like data science and computer science as my school subjects\".lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('why', 'WRB'),\n",
       " ('is', 'BEZ'),\n",
       " ('the', 'AT'),\n",
       " ('google', 'VBZ'),\n",
       " ('podcasts', 'VBZ'),\n",
       " ('app', 'NP'),\n",
       " ('failing', 'VBG'),\n",
       " ('so', 'RB'),\n",
       " ('hard', 'JJ'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et2.tag(nltk.word_tokenize(\"Why Is the Google Podcasts App Failing So Hard?\".lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import StanfordParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyParser(StanfordParser):\n",
    "\n",
    "        def raw_parse_sents(self, sentences, verbose=False):\n",
    "            \"\"\"\n",
    "            Use StanfordParser to parse multiple sentences. Takes multiple sentences as a\n",
    "            list of strings.\n",
    "            Each sentence will be automatically tokenized and tagged by the Stanford Parser.\n",
    "            The output format is `wordsAndTags`.\n",
    "\n",
    "            :param sentences: Input sentences to parse\n",
    "            :type sentences: list(str)\n",
    "            :rtype: iter(iter(Tree))\n",
    "            \"\"\"\n",
    "            cmd = [\n",
    "                self._MAIN_CLASS,\n",
    "                '-model', self.model_path,\n",
    "                '-sentences', 'newline',\n",
    "                '-outputFormat', 'wordsAndTags',\n",
    "            ]\n",
    "            return self._parse_trees_output(self._execute(cmd, '\\n'.join(sentences), verbose))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018 = getDataframe(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go proposal for blockchain support in standard library\n",
      "[('go', 'VB'), ('proposal', 'NN'), ('for', 'IN'), ('blockchain', None), ('support', 'NN'), ('in', 'IN'), ('standard', 'JJ'), ('library', 'NN')]\n",
      "***************************************POS_TAG***************************************\n",
      "[('go', 'VB'), ('proposal', 'NN'), ('for', 'IN'), ('blockchain', 'JJ'), ('support', 'NN'), ('in', 'IN'), ('standard', 'JJ'), ('library', 'NN')]\n",
      "Test of Skylake-X, Cannonlake and Goldmont\n",
      "[('test', 'NN-HL'), ('of', 'IN-HL'), ('skylake-x', None), (',', ','), ('cannonlake', None), ('and', 'CC'), ('goldmont', None)]\n",
      "***************************************POS_TAG***************************************\n",
      "[('test', 'NN'), ('of', 'IN'), ('skylake-x', 'NN'), (',', ','), ('cannonlake', 'NN'), ('and', 'CC'), ('goldmont', 'NN')]\n",
      "Yubico Open Source SDK: Securing Infrastructures, Cryptographic Key Material\n",
      "[('yubico', None), ('open', 'JJ'), ('source', 'NN'), ('sdk', None), (':', ':'), ('securing', 'VBG'), ('infrastructures', None), (',', ','), ('cryptographic', 'JJ'), ('key', 'NN'), ('material', 'NN')]\n",
      "***************************************POS_TAG***************************************\n",
      "[('yubico', 'NN'), ('open', 'JJ'), ('source', 'NN'), ('sdk', 'NN'), (':', ':'), ('securing', 'NN'), ('infrastructures', 'NNS'), (',', ','), ('cryptographic', 'JJ'), ('key', 'JJ'), ('material', 'NN')]\n",
      "The Death of Microservice Madness in 2018\n",
      "[('the', 'AT'), ('death', 'NN'), ('of', 'IN'), ('microservice', None), ('madness', 'NN'), ('in', 'IN'), ('2018', 'CD')]\n",
      "***************************************POS_TAG***************************************\n",
      "[('the', 'DT'), ('death', 'NN'), ('of', 'IN'), ('microservice', 'NN'), ('madness', 'NN'), ('in', 'IN'), ('2018', 'CD')]\n",
      "How to use VoiceOver on iPhone and iPad\n",
      "[('how', 'WRB'), ('to', 'TO'), ('use', 'VB'), ('voiceover', None), ('on', 'IN'), ('iphone', None), ('and', 'CC'), ('ipad', None)]\n",
      "***************************************POS_TAG***************************************\n",
      "[('how', 'WRB'), ('to', 'TO'), ('use', 'VB'), ('voiceover', 'NN'), ('on', 'IN'), ('iphone', 'NN'), ('and', 'CC'), ('ipad', 'NN')]\n",
      "Knowing Your Way in Remote Places\n",
      "[('knowing', 'VBG'), ('your', 'PP$'), ('way', 'NN'), ('in', 'IN'), ('remote', 'JJ'), ('places', 'NNS')]\n",
      "***************************************POS_TAG***************************************\n",
      "[('knowing', 'VBG'), ('your', 'PRP$'), ('way', 'NN'), ('in', 'IN'), ('remote', 'JJ'), ('places', 'NNS')]\n",
      "In defense of letting children quit\n",
      "[('in', 'IN'), ('defense', 'NN'), ('of', 'IN'), ('letting', 'VBG'), ('children', 'NNS'), ('quit', 'VB')]\n",
      "***************************************POS_TAG***************************************\n",
      "[('in', 'IN'), ('defense', 'NN'), ('of', 'IN'), ('letting', 'VBG'), ('children', 'NNS'), ('quit', 'NN')]\n",
      "How we solved our office Wi-Fi problems\n",
      "[('how', 'WRB'), ('we', 'PPSS'), ('solved', 'VBN'), ('our', 'PP$'), ('office', 'NN'), ('wi-fi', None), ('problems', 'NNS')]\n",
      "***************************************POS_TAG***************************************\n",
      "[('how', 'WRB'), ('we', 'PRP'), ('solved', 'VBD'), ('our', 'PRP$'), ('office', 'NN'), ('wi-fi', 'NN'), ('problems', 'NNS')]\n",
      "Say goodbye to the information age: it’s all about reputation now\n",
      "[('say', 'VB'), ('goodbye', 'UH'), ('to', 'TO'), ('the', 'DT'), ('information', 'NN'), ('age', 'NN'), (':', ':'), ('it', 'PRP'), ('’', None), ('s', 'POS'), ('all', 'ABN'), ('about', 'IN'), ('reputation', 'NN'), ('now', 'RB')]\n",
      "***************************************POS_TAG***************************************\n",
      "[('say', 'VB'), ('goodbye', 'NN'), ('to', 'TO'), ('the', 'DT'), ('information', 'NN'), ('age', 'NN'), (':', ':'), ('it', 'PRP'), ('’', 'VBZ'), ('s', 'RB'), ('all', 'DT'), ('about', 'IN'), ('reputation', 'NN'), ('now', 'RB')]\n",
      "MusicBrainz Picard 2.0 released\n",
      "[('musicbrainz', None), ('picard', None), ('2.0', 'CD'), ('released', 'VBN')]\n",
      "***************************************POS_TAG***************************************\n",
      "[('musicbrainz', 'RB'), ('picard', 'RB'), ('2.0', 'CD'), ('released', 'VBD')]\n"
     ]
    }
   ],
   "source": [
    "for sent in df_2018['Title'][np.random.randint(len(df_2018), size=10)]:\n",
    "    print(sent)\n",
    "    print(et2.tag(nltk.word_tokenize(sent.lower())))\n",
    "    print('***************************************POS_TAG***************************************')\n",
    "    print(nltk.pos_tag(nltk.word_tokenize(sent.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xybubez', 'NN')]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag([\"xybubez\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank,conll2000,nps_chat,framenet,reuters,propbank_ptb,wordnet,CategorizedTaggedCorpusReader\n",
    "tree_sents = treebank.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3914"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tree_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57340"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10567"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nps_chat.tagged_posts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'nltk.corpus' has no attribute 'book_grammars'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-44b3d570c578>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook_grammars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagged_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.5/site-packages/nltk/lazyimport.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    120\u001b[0m                   'Module load triggered by attribute %r read access' % name)\n\u001b[1;32m    121\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__lazymodule_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'nltk.corpus' has no attribute 'book_grammars'"
     ]
    }
   ],
   "source": [
    "(nltk.corpus.book_grammars.tagged_sents()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = treebank.tagged_sents() +brown.tagged_sents() + conll2000.tagged_sents() + nps_chat.tagged_posts() + nltk.corpus.conll2002.tagged_sents() + nltk.corpus.conll2007.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_train = []\n",
    "for sent in new_train:\n",
    "    edited_train.append([(word.lower(),tag) for (word,tag) in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125107"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "et1 = UnigramTagger(edited_train, backoff = None)\n",
    "et2 = BigramTagger(edited_train, backoff = et1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', 'NNS'),\n",
       " ('science', 'NN'),\n",
       " ('is', 'BEZ'),\n",
       " ('a', 'AT'),\n",
       " ('good', 'JJ'),\n",
       " ('subject', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('computer', 'NN'),\n",
       " ('science', 'NN'),\n",
       " ('adds', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('overall', 'JJ'),\n",
       " ('fun', 'NN')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et2.tag(nltk.word_tokenize(\" Data science is a good subject and Computer Science adds to the overall fun\".lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penn_to_wn(tag):\n",
    "    nltk_wn_pos = {'J':wn.ADJ,'V':wn.VERB,'N':wn.NOUN,'R':wn.ADV}\n",
    "    try:\n",
    "        return nltk_wn_pos[tag[0]]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def checkPunctuation(word):\n",
    "    punctuations = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/','”','“','–',\"'s\",\n",
    "                ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
    "    if word in punctuations:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def tokenizeSentence_1(sentence):\n",
    "    tokenized = []\n",
    "    text = nltk.word_tokenize(sentence)\n",
    "    grab = False\n",
    "    print(nltk.pos_tag(text))\n",
    "    for word,pos in nltk.pos_tag(text):\n",
    "        if checkPunctuation(word):\n",
    "            continue\n",
    "        tag = penn_to_wn(pos)\n",
    "        if tag is None:\n",
    "            lemmatizedWord = word\n",
    "        else:\n",
    "            lemmatizedWord = wordnet_lemmatizer.lemmatize(word,tag)\n",
    "        lemmatizedWord = lemmatizedWord.lower()\n",
    "        if pos=='NNP':\n",
    "            if grab:\n",
    "                tokenized[-1] += \" \" + lemmatizedWord\n",
    "                grab = False\n",
    "                continue\n",
    "            else:\n",
    "                grab = True\n",
    "        else:\n",
    "            if grab:\n",
    "                grab = False\n",
    "        tokenized.append(lemmatizedWord)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penn_to_wn(tag):\n",
    "    nltk_wn_pos = {'J':wn.ADJ,'V':wn.VERB,'N':wn.NOUN,'R':wn.ADV}\n",
    "    try:\n",
    "        return nltk_wn_pos[tag[0]]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def checkPunctuation(word):\n",
    "    punctuations = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/','”','“','–',\"'s\",\n",
    "                ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
    "    if word in punctuations:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def tokenizeSentence(sentence):\n",
    "    tokenized = []\n",
    "    text = nltk.word_tokenize(sentence)\n",
    "    grab = False\n",
    "    print(nltk.pos_tag(text))\n",
    "    for word,pos in et2.tag(text):\n",
    "        if checkPunctuation(word):\n",
    "            continue\n",
    "        tag = penn_to_wn(pos)\n",
    "        if tag is None:\n",
    "            lemmatizedWord = word\n",
    "        else:\n",
    "            lemmatizedWord = wordnet_lemmatizer.lemmatize(word,tag)\n",
    "        lemmatizedWord = lemmatizedWord.lower()\n",
    "        if pos=='NN':\n",
    "            if grab:\n",
    "                tokenized[-1] += \" \" + lemmatizedWord\n",
    "                grab = False\n",
    "                continue\n",
    "            else:\n",
    "                grab = True\n",
    "        else:\n",
    "            if grab:\n",
    "                grab = False\n",
    "        tokenized.append(lemmatizedWord)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text  = '''If you're in construction or need to pass fire inspection, or just want fire resistant materials for peace of mind, this is the one to use. Check out 3rd party sellers as well Skylite'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  If/IN\n",
      "  you/PRP\n",
      "  (RELATION 're/VBP)\n",
      "  in/IN\n",
      "  (NP construction/NN)\n",
      "  or/CC\n",
      "  (NP need/NN)\n",
      "  to/TO\n",
      "  (RELATION pass/VB)\n",
      "  (NP fire/NN inspection/NN)\n",
      "  ,/,\n",
      "  or/CC\n",
      "  just/RB\n",
      "  (RELATION want/VB)\n",
      "  (NP fire/NN)\n",
      "  (NP resistant/JJ materials/NNS)\n",
      "  for/IN\n",
      "  (NP peace/NN)\n",
      "  of/IN\n",
      "  (NP mind/NN)\n",
      "  ,/,\n",
      "  this/DT\n",
      "  (RELATION is/VBZ)\n",
      "  (NP the/DT one/NN)\n",
      "  to/TO\n",
      "  (RELATION use/VB)\n",
      "  ./.)\n",
      "<class 'nltk.tree.Tree'>\n",
      "(S\n",
      "  (RELATION Check/VB)\n",
      "  out/RP\n",
      "  3rd/CD\n",
      "  (NP party/NN sellers/NNS)\n",
      "  as/IN\n",
      "  well/RB\n",
      "  (NP Skylite/NNP))\n",
      "<class 'nltk.tree.Tree'>\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "\n",
    "grammar = \"\"\"NP: {<DT>?<JJ>*<NN.*>+}\n",
    "       RELATION: {<V.*>}\n",
    "                 {<DT>?<JJ>*<NN.*>+}\n",
    "       ENTITY: {<NN.*>}\"\"\"\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "for i in sentences:\n",
    "    result = cp.parse(i)\n",
    "    print(result)\n",
    "    print(type(result))\n",
    "    result.draw() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
